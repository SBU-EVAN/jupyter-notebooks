{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951dcfdf",
   "metadata": {},
   "source": [
    "## Joint MCMC Sampling\n",
    "This notebook attemps to sample from the joint posterior of planck and LSST. The emulators are used to calculate the likelihood. For parameters in planck but not LSST the joint prior is extended to be uninformative in the new parameters. Since the experiments are independent the joint likelihood is simply the sum/product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f153718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from getdist import plots, MCSamples\n",
    "import getdist\n",
    "from multiprocessing import Pool\n",
    "from getdist import plots, MCSamples, WeightedSamples\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '../'))\n",
    "# LSST\n",
    "from LSST_emulation.cocoa_emu import *\n",
    "from LSST_emulation.cocoa_emu.emulator import NNEmulator, GPEmulator\n",
    "from LSST_emulation.cocoa_emu.data_model import LSST_3x2\n",
    "\n",
    "# cosmopower\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '../'))\n",
    "import cosmopower.likelihoods.tf_planck2018_lite as cppl\n",
    "import cosmopower as cp\n",
    "\n",
    "import emcee\n",
    "import time\n",
    "\n",
    "# Now normalizing flow\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "tfd = tfp.distributions\n",
    "tfk = tf.keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from numpy import linalg\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59e98079",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Planck MCMC\n",
    "ipynb_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "\n",
    "\n",
    "tt_emu_model = cp.cosmopower_NN(restore=True,\n",
    "                                restore_filename=os.path.join(ipynb_path, '../cosmopower_emcee/cosmopower/trained_models/CP_paper/CMB/cmb_TT_NN')\n",
    "                               )\n",
    "\n",
    "te_emu_model = cp.cosmopower_PCAplusNN(restore=True,\n",
    "                                restore_filename=os.path.join(ipynb_path, '../cosmopower_emcee/cosmopower/trained_models/CP_paper/CMB/cmb_TE_PCAplusNN')\n",
    "                               )\n",
    "\n",
    "ee_emu_model = cp.cosmopower_NN(restore=True,\n",
    "                                restore_filename=os.path.join(ipynb_path, '../cosmopower_emcee/cosmopower/trained_models/CP_paper/CMB/cmb_EE_NN')\n",
    "                               )\n",
    "\n",
    "# path to the tf_planck2018_lite likelihood\n",
    "#tf_planck2018_lite_path = './../cosmopower/likelihoods/tf_planck2018_lite/'\n",
    "\n",
    "# parameters of the analysis, and their priors\n",
    "parameters_and_priors = {'omega_b':      [0.001, 0.04, 'uniform'],\n",
    "                         'omega_cdm':    [0.005, 0.99,  'uniform'],\n",
    "                         'h':            [0.2,   1.0,   'uniform'],\n",
    "                         'tau_reio':     [0.01,  0.8,   'uniform'],\n",
    "                         'n_s':          [0.9,   1.1,   'uniform'],\n",
    "                         'ln10^{10}A_s': [1.61,  3.91,  'uniform'],\n",
    "                         'A_planck':     [1.0,   0.01,  'gaussian'],\n",
    "                          }\n",
    "\n",
    "# instantiation\n",
    "#tf_planck = cp.tf_planck2018_lite(parameters=parameters_and_priors, \n",
    "#                                  tf_planck2018_lite_path=tf_planck2018_lite_path,\n",
    "#                                  tt_emu_model=tt_emu_model,\n",
    "#                                  te_emu_model=te_emu_model,\n",
    "#                                  ee_emu_model=ee_emu_model\n",
    "#                                  )\n",
    "\n",
    "# initial points\n",
    "FIDUCIAL = np.reshape(np.array([ 0.022242,  0.11977,  0.673,  0.055,  0.9658,  3.0753,  1.0080]).astype('float32'), (1, 7))\n",
    "EPSILON  = np.reshape(np.array([ 1E-4,      1E-4,     1E-2,   1E-3,   1E-3,    1E-3,    1E-4]).astype('float32'), (1,7))\n",
    "\n",
    "# instantiation\n",
    "tf_planck = cppl(parameters=parameters_and_priors, \n",
    "                                  tf_planck2018_lite_path='/home/grads/data/evan/cosmopower_emcee/cosmopower/likelihoods/tf_planck2018_lite',\n",
    "                                  tt_emu_model=tt_emu_model,\n",
    "                                  te_emu_model=te_emu_model,\n",
    "                                  ee_emu_model=ee_emu_model\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4711007",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSST MCMC Functions\n",
    "def add_bias(bias_theta, datavector):\n",
    "    for i in range(5):\n",
    "        factor = (bias_theta[i] / bias_fid[i])**bias_mask[i]\n",
    "        datavector = factor * datavector\n",
    "    return datavector\n",
    "\n",
    "def add_shear_calib(m, datavector):\n",
    "    for i in range(5):\n",
    "        factor = (1 + m[i])**shear_calib_mask[i]\n",
    "        datavector = factor * datavector\n",
    "    return datavector\n",
    "\n",
    "def hard_prior(theta, params_prior):\n",
    "    \"\"\"\n",
    "    A function to impose a flat prior on a set of parameters.\n",
    "    :theta: The set of parameter values\n",
    "    :params_prior: The minimum and the maximum value of the parameters on which this prior is imposed\n",
    "    \"\"\"\n",
    "    is_lower_than_min = bool(np.sum(theta < params_prior[:,0]))\n",
    "    is_higher_than_max = bool(np.sum(theta > params_prior[:,1]))\n",
    "    if is_lower_than_min or is_higher_than_max:\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return 0.\n",
    "    \n",
    "cosmo_prior_lim = np.array([[1.61, 3.91],# planck [1.61,  3.91,  'uniform'],\n",
    "                       [0.87, 1.07], # planck [0.9,   1.1,   'uniform']\n",
    "                       [55, 91],\n",
    "                       [0.01, 0.04], # [0.001, 0.04, 'uniform'],\n",
    "                       [0.001, 0.99]]) # [0.005, 0.99,  'uniform'],\n",
    "\n",
    "# parameters of the analysis, and their priors\n",
    "\n",
    "ia_prior_lim = np.array([[-5., 5.],\n",
    "                       [-5., 5.]])\n",
    "\n",
    "bias_prior_lim = np.array([[0.8, 3.],\n",
    "                       [0.8, 3.],\n",
    "                       [0.8, 3.],\n",
    "                       [0.8, 3.],\n",
    "                       [0.8, 3.]])\n",
    "\n",
    "baryon_prior_lim = np.array([[-3., 12.],\n",
    "                             [-2.5, 2.5]])\n",
    "\n",
    "baryon_prior_lim = 3. * baryon_prior_lim \n",
    "\n",
    "dz_source_std   = 0.002 * np.ones(5)\n",
    "dz_lens_std     = 0.005 * np.ones(5)\n",
    "shear_calib_std = 0.005 * np.ones(5)\n",
    "    \n",
    "def lnprior(theta):\n",
    "    cosmo_theta = theta[:5]\n",
    "    ns          = cosmo_theta[1]\n",
    "\n",
    "    ns_prior    = 0.\n",
    "    \n",
    "    dz_source   = theta[5:10]\n",
    "    ia_theta    = theta[10:12]\n",
    "    dz_lens     = theta[12:17]\n",
    "    bias        = theta[17:22]\n",
    "    shear_calib = theta[22:27]\n",
    "    baryon_q    = theta[27:29]\n",
    "    \n",
    "    cosmo_prior = hard_prior(cosmo_theta, cosmo_prior_lim) + ns_prior\n",
    "    ia_prior    = hard_prior(ia_theta, ia_prior_lim)\n",
    "    bias_prior  = hard_prior(bias, bias_prior_lim)\n",
    "    baryon_prior = hard_prior(baryon_q, baryon_prior_lim)\n",
    "    \n",
    "    dz_source_lnprior   = -0.5 * np.sum((dz_source / dz_source_std)**2)\n",
    "    dz_lens_lnprior     = -0.5 * np.sum((dz_lens / dz_lens_std)**2)\n",
    "    shear_calib_lnprior = -0.5 * np.sum((shear_calib / shear_calib_std)**2)\n",
    "    \n",
    "    return cosmo_prior + ia_prior + dz_source_lnprior + dz_lens_lnprior + \\\n",
    "            shear_calib_lnprior + bias_prior + baryon_prior\n",
    "    \n",
    "def ln_lkl(theta):\n",
    "    model_datavector = get_data_vector_emu(theta)\n",
    "    delta_dv = (model_datavector - data_model.dv_obs)[data_model.mask_3x2]\n",
    "    return -0.5 * delta_dv @ data_model.masked_inv_cov @ delta_dv\n",
    "\n",
    "def get_data_vector_emu(theta):\n",
    "    \"\"\"\n",
    "    Function to get the emulated data vector (including the effect of galaxy bias, baryons, etc.)\n",
    "    \"\"\"\n",
    "    cosmo_ia_dz_theta = theta[:17]\n",
    "    bias        = theta[17:22]\n",
    "    shear_calib = theta[22:27]\n",
    "    baryon_q    = theta[27:29]\n",
    "    datavector = data_model.compute_datavector(cosmo_ia_dz_theta)\n",
    "    datavector = np.array(datavector)\n",
    "    datavector = add_bias(bias, datavector)\n",
    "    datavector = add_shear_calib(shear_calib, datavector)\n",
    "    return datavector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5656918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Planck MCMC functions\n",
    "#\n",
    "# from LSST emulator for index reference\n",
    "# names = ['logA', 'ns', 'H0', 'omegab', 'omegac']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5e1975",
   "metadata": {},
   "source": [
    "As a reminder, the parameters are as follows:\n",
    "- LSST: labels =  ['logA', \n",
    "        'ns', \n",
    "        'H0', \n",
    "        'omega b', \n",
    "        'omega c'] \n",
    "        in [:,:5]\n",
    "- Planck: names = [r\"\\omega_{\\mathrm{b}}\", \n",
    "         r\"\\omega_{\\mathrm{cdm}}\", \n",
    "         r\"h\", \n",
    "         r\"\\tau\",\n",
    "         r\"n_s\", \n",
    "         r\"ln 10^{10} A_s\", \n",
    "         r\"A_{\\mathrm{Planck}}\"]\n",
    "         \n",
    "They have in common the densities $\\Omega_b$ and $\\Omega_c$, the spectral index $n_s$, the amplitude $A_s$. This means we must extend the LSST the priors in 3 extra dimensions, one for $\\tau$, one for $h$, and one for the planck calibration amplitude $A_{\\mathrm{Planck}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7614e1d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cocoa_emu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1288453/664894844.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0memu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNEmulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv_fid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0memu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'../../model/nn_emu/model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;31m# ======================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/evan/LSST_emulation/cocoa_emu/emulator/nn_emulator.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mh5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cocoa_emu'"
     ]
    }
   ],
   "source": [
    "# open the emulator\n",
    "# Get the LSST covariance and fid data\n",
    "path = '/home/grads/data/evan/LSST_emulation/data/lsst_y1/'\n",
    "lsst_cov = np.loadtxt(path+'cov_lsst_y1')\n",
    "fid_cos = np.loadtxt(path+'lsst_y1_data_fid',dtype=np.float32)[:,1]\n",
    "\n",
    "lsst_y1_cov = np.zeros((1560, 1560))\n",
    "for line in lsst_cov:\n",
    "    i = int(line[0])\n",
    "    j = int(line[1])\n",
    "\n",
    "    cov_g_block  = line[-2]\n",
    "    cov_ng_block = line[-1]\n",
    "\n",
    "    cov_ij = cov_g_block + cov_ng_block\n",
    "\n",
    "    lsst_y1_cov[i,j] = cov_ij\n",
    "    lsst_y1_cov[j,i] = cov_ij\n",
    "    \n",
    "fid = torch.Tensor(fid_cos)\n",
    "cov = torch.Tensor(lsst_y1_cov)\n",
    "\n",
    "configfile = path+'../../configs/nn_emu.yaml'\n",
    "config = Config(configfile)\n",
    "\n",
    "config_args     = config.config_args\n",
    "config_args_io  = config_args['io']\n",
    "config_args_data = config_args['data']\n",
    "\n",
    "savedir = path+'../../output/nn_emu/'\n",
    "\n",
    "N_DIM         = 17\n",
    "data_model    = LSST_3x2(N_DIM, config_args_io, config_args_data)\n",
    "data_model.emu_type = 'nn'\n",
    "OUTPUT_DIM = 1560\n",
    "\n",
    "emu = NNEmulator(N_DIM, OUTPUT_DIM, data_model.dv_fid, data_model.dv_std)    \n",
    "emu.load(path+'../../model/nn_emu/model')\n",
    "# ======================================================\n",
    "\n",
    "data_model.emu = emu\n",
    "\n",
    "bias_fid         = data_model.bias_fid\n",
    "bias_mask        = data_model.bias_mask\n",
    "shear_calib_mask = data_model.shear_calib_mask\n",
    "\n",
    "N_MCMC        = 5000\n",
    "N_WALKERS     = 120\n",
    "NDIM_SAMPLING = 32\n",
    "\n",
    "theta0    = np.array([3.0675, 0.97, 69.0, 0.0228528, 0.1199772, \n",
    "                      0., 0., 0., 0., 0.,\n",
    "                      0.5, 0.,\n",
    "                      0., 0., 0., 0., 0.,\n",
    "                      1.24, 1.36, 1.47, 1.60, 1.76,\n",
    "                      0., 0., 0., 0., 0.,\n",
    "                      0., 0.,1.05,0.05,1.0])\n",
    "\n",
    "theta_std = np.array([0.01, 0.001, 0.1, 0.001, 0.002, \n",
    "                      0.002, 0.002, 0.002, 0.002, 0.002, \n",
    "                      0.1, 0.1,\n",
    "                      0.005, 0.005, 0.005, 0.005, 0.005, \n",
    "                      0.03, 0.03, 0.03, 0.03, 0.03,\n",
    "                      0.005, 0.005, 0.005, 0.005, 0.005, \n",
    "                      0.1, 0.1,0.01,0.01,0.01]) \n",
    "\n",
    "# Starting position of the emcee chain\n",
    "pos0 = theta0[np.newaxis] + 3. * theta_std[np.newaxis] * np.random.normal(size=(N_WALKERS, NDIM_SAMPLING))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbba756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planck_ln_lkl(theta):\n",
    "    theta_p = np.zeros(6)\n",
    "    theta_p[0] = theta[3] # omega b\n",
    "    theta_p[1] = theta[4] # omega c\n",
    "    theta_p[2] = theta[29] # h\n",
    "    theta_p[3] = theta[30] # tau\n",
    "    theta_p[4] = theta[0] # A_s\n",
    "    theta_p[5] = theta[31] # A_planck\n",
    "    p=tf_planck.posterior(theta_p.astype(np.float32)).numpy()\n",
    "    return p    \n",
    "\n",
    "def lnprob(theta):\n",
    "    return lnprior(theta) + ln_lkl(theta) + planck_ln_lkl(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ec823",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#with Pool(10) as pool:\n",
    "emu_sampler = emcee.EnsembleSampler(N_WALKERS, NDIM_SAMPLING+6, lnprob)#, pool=pool)\n",
    "emu_sampler.run_mcmc(pos0, N_MCMC, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0532dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emu",
   "language": "python",
   "name": "emu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
